{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google drive & Wandb"
      ],
      "metadata": {
        "id": "a0r_0ZMLLQhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#!pip install wandb\n",
        "#import wandb"
      ],
      "metadata": {
        "id": "fxCt-0Oms6Ae"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions\n"
      ],
      "metadata": {
        "id": "SBUlUS58Yz2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Encoding the GCD paper used. For example, 10 and 150 as [+,1,0,+,1,5,0]\n",
        "def encode_feature(a, b):\n",
        "    encoded = [plus_sign_label] + [int(d) for d in str(a)] + [plus_sign_label] + [int(d) for d in str(b)]\n",
        "    return encoded\n",
        "\n",
        "# Data generation [[input],[target]]\n",
        "def generate_gcd_encoded_data(num_samples, max_value,log_uniform=True):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        # Sampling from log-uniform distribution suggested by the GCD paper\n",
        "        if log_uniform :\n",
        "            a = int(math.exp(random.uniform(0, math.log(max_value))))\n",
        "            b = int(math.exp(random.uniform(0, math.log(max_value))))\n",
        "        else :\n",
        "            a = random.randint(0, max_value)\n",
        "            b = random.randint(0, max_value)\n",
        "\n",
        "        gcd_value = math.gcd(a, b)\n",
        "        encoded_gcd = [plus_sign_label] + [int(d) for d in str(gcd_value)]\n",
        "        data.append((encode_feature(a, b), encoded_gcd))\n",
        "    return data\n",
        "\n",
        "# Naive padding to fixed length\n",
        "def pad_sequence(seq, max_len):\n",
        "    return seq + ([padding_label] * (max_len - len(seq)))\n",
        "\n",
        "# Custom Dataset\n",
        "class GCDDataset(Dataset):\n",
        "    def __init__(self, data, max_len):\n",
        "        self.data = [(pad_sequence(seq, max_len), pad_sequence(target, max_len)) for seq, target in data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, target = self.data[idx]\n",
        "        return torch.tensor(seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "\"\"\"\n",
        "# Resume or create a new graph\n",
        "def init_wandb(resume, name, resume_run_id=None):\n",
        "  if not resume:\n",
        "    new_run = wandb.init(project=\"gcd\", name=name, resume=\"allow\")\n",
        "    run_id = new_run.id\n",
        "    print(f'\\n##### RUN ID: {run_id} #####')\n",
        "    return\n",
        "\n",
        "  wandb.init(project=\"gcd\", name=name, resume=\"allow\", id=resume_run_id)\n",
        "  return\n",
        "\n",
        "# Others\n",
        "\n",
        "def wandb_log(train_loss,stratified_test_loss):\n",
        "   wandb.log({\n",
        "            \"train_loss\": train_loss,\n",
        "            \"stratified_test_loss\": stratified_test_loss,\n",
        "            })\n",
        "\n",
        "def save_checkpoint(epoch_num,model,optimizer):\n",
        "  torch.save({\n",
        "            'epoch': epoch_num,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\"\"\"\n",
        "\n",
        "def print_first_element(loader):\n",
        "    for sequences, targets in loader:\n",
        "        print(\"First Sequence in Batch:\", sequences[0].numpy())\n",
        "        print(\"First Target in Batch:\", targets[0].numpy())\n",
        "        break"
      ],
      "metadata": {
        "id": "AA7hqKu6Ywat"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Evaluates the model's predictions by calculating the accuracy of predicted sequences\n",
        "    against true sequences, excluding padding and start tokens. The function aggregates\n",
        "    sequences of tokens into numerical values for direct comparison \"\"\"\n",
        "\n",
        "def report(model, loader, acc=True, freq=False):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    total_correct_predictions_count = 0\n",
        "    total_predictions_count = 0\n",
        "    correct_predictions_count = {}\n",
        "    all_aggregated_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            src, tgt = data[0].to(device), data[1].to(device)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "\n",
        "            src_key_padding_mask = (src == padding_label).to(device)\n",
        "            tgt_key_padding_mask = (tgt_input == padding_label).to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                src,\n",
        "                tgt_input,\n",
        "                src_key_padding_mask=src_key_padding_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                tgt_mask=tgt_mask\n",
        "            )\n",
        "\n",
        "            _, predicted = torch.max(outputs, dim=2)\n",
        "            tgt_expected = tgt[:, 1:].reshape(-1)\n",
        "            predicted = predicted.reshape(-1)\n",
        "\n",
        "            # Filter out padding tokens from the calculation\n",
        "            non_padding_mask = tgt_expected != padding_label\n",
        "            valid_predictions = predicted[non_padding_mask]\n",
        "            valid_labels = tgt_expected[non_padding_mask]\n",
        "\n",
        "            # Lists to store the aggregated numbers from predictions and labels\n",
        "            pred_numbers = []\n",
        "            true_numbers = []\n",
        "            start_idx = 0\n",
        "\n",
        "            # Calculate the lengths of the sequences without padding\n",
        "            seq_lens = (tgt.size(1) - 1) - tgt[:, 1:].eq(padding_label).sum(dim=1)\n",
        "\n",
        "            # Loop over the lengths of the sequences to extract and compare numbers\n",
        "            for seq_len in seq_lens:\n",
        "                # Calculate the end index for the current sequence based on its length\n",
        "                end_idx = start_idx + seq_len.item()\n",
        "\n",
        "                # Extract the sequence of predictions and labels as lists\n",
        "                seq_pred = valid_predictions[start_idx:end_idx].tolist()\n",
        "                seq_true = valid_labels[start_idx:end_idx].tolist()\n",
        "\n",
        "                # Convert the lists of digits into integers representing the aggregated number\n",
        "                pred_number = int(''.join(map(str, seq_pred)))\n",
        "                true_number = int(''.join(map(str, seq_true)))\n",
        "\n",
        "                # Append the aggregated numbers to the respective lists\n",
        "                pred_numbers.append(pred_number)\n",
        "                true_numbers.append(true_number)\n",
        "\n",
        "                # Keep track of all predicted numbers\n",
        "                all_aggregated_predictions.append(pred_number)\n",
        "\n",
        "                if pred_number == true_number:\n",
        "                    total_correct_predictions_count += 1\n",
        "                    correct_predictions_count[pred_number] = correct_predictions_count.get(pred_number, 0) + 1\n",
        "\n",
        "                # Update the start index for the next sequence\n",
        "                start_idx = end_idx\n",
        "\n",
        "            total_predictions_count += len(pred_numbers)\n",
        "\n",
        "    if acc:\n",
        "        overall_accuracy = 100 * total_correct_predictions_count / total_predictions_count if total_predictions_count > 0 else 0\n",
        "        print(f'Overall Accuracy: {overall_accuracy}%\\n')\n",
        "\n",
        "    if freq:\n",
        "        print(\"Accuracy for each unique predicted number:\")\n",
        "        for num in set(all_aggregated_predictions):\n",
        "            correct_count = correct_predictions_count.get(num, 0)\n",
        "            total_count = all_aggregated_predictions.count(num)\n",
        "            accuracy = 100 * correct_count / total_count if total_count > 0 else 0\n",
        "            print(f'Accuracy for number {num}: {accuracy:.2f}% (Correct: {correct_count}, Total Prediction: {total_count})')"
      ],
      "metadata": {
        "id": "f1L075tLRD8D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "3otgiDkNaweq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plus_sign_label = 10\n",
        "padding_label = 11\n",
        "\n",
        "sample_size = int(4e5)\n",
        "max_value = int(1e7) # upperbound of data generation\n",
        "\n",
        "# limited training set around 40% suggest by Neel for grokking\n",
        "test_size = 0.5\n",
        "batch_size = 256\n",
        "\n",
        "# padding to max_len\n",
        "max_num_length = len(str(max_value))\n",
        "max_len = (max_num_length + 1) * 2\n",
        "\n",
        "#checkpoint_path = '/content/drive/MyDrive/CSCI567/2layers_256d_8heads_lr1e-4.pth'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NVNO-FGTayQy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "0AVYWiWztIzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(52)\n",
        "random_state = 52\n",
        "\n",
        "raw_data = generate_gcd_encoded_data(sample_size, max_value)\n",
        "train_data, test_data = train_test_split(raw_data, test_size=test_size, random_state=random_state)\n",
        "train_dataset = GCDDataset(train_data, max_len)\n",
        "test_dataset = GCDDataset(test_data, max_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size , shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size , shuffle=False)\n",
        "\n",
        "print(\"Train Loader:\")\n",
        "print_first_element(train_loader)\n",
        "print(\"\\n\")\n",
        "print(\"Test Loader:\")\n",
        "print_first_element(test_loader)"
      ],
      "metadata": {
        "id": "CL0gdcjY162K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_coprime_pair(max_value):\n",
        "    while True:\n",
        "        a = random.randint(1, max_value)\n",
        "        b = random.randint(1, max_value)\n",
        "        if math.gcd(a, b) == 1:\n",
        "            return a, b\n",
        "\n",
        "# Stratified test data suggested by GCD paper, accuraccy on normal test set for GCD\n",
        "# is misleading\n",
        "def generate_stratified_gcd_test_set(num_samples_per_k, max_value, k_max=100):\n",
        "    stratified_test_data = []\n",
        "    for k in range(1, k_max + 1):\n",
        "        for _ in range(num_samples_per_k):\n",
        "            a, b = generate_coprime_pair(max_value // k)\n",
        "            a *= k\n",
        "            b *= k\n",
        "            gcd_value = k\n",
        "            encoded_pair = encode_feature(a, b)\n",
        "            encoded_gcd = [plus_sign_label] + [int(d) for d in str(gcd_value)]\n",
        "            stratified_test_data.append((encoded_pair, encoded_gcd))\n",
        "    return stratified_test_data\n",
        "\n",
        "stratified_test_set = generate_stratified_gcd_test_set(num_samples_per_k=1000, max_value=max_value, k_max=100)\n",
        "stratified_dataset = GCDDataset(stratified_test_set , max_len)\n",
        "stratified_test_loader = DataLoader(stratified_dataset, batch_size=batch_size , shuffle=False)\n",
        "print(\"Stratified Test Loader:\")\n",
        "print_first_element(stratified_test_loader)"
      ],
      "metadata": {
        "id": "wDlMa1A5mxdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model constructor"
      ],
      "metadata": {
        "id": "1evOv1H9Qmpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# conventional seq2seq auto-regressive Transformer with positional encoding\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model=512, nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=2048,dropout=0.0):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, d_model, padding_idx=padding_label)\n",
        "        self.positional_encoding = self.create_positional_encoding(max_len, d_model)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers,enable_nested_tensor=False)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    # From Pytorch doc\n",
        "    def create_positional_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        return nn.Parameter(pe, requires_grad=False)\n",
        "\n",
        "    def forward(self, src, tgt, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None,memory_key_padding_mask=None):\n",
        "      # Attn all you need paper used this\n",
        "      src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "      tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "\n",
        "      # Add positional encoding to src and tgt embeddings\n",
        "      src = src + self.positional_encoding[:, :src.size(1), :]\n",
        "      tgt = tgt + self.positional_encoding[:, :tgt.size(1), :]\n",
        "\n",
        "      memory = self.transformer_encoder(src.transpose(0, 1), src_key_padding_mask=src_key_padding_mask)\n",
        "      output = self.transformer_decoder(tgt.transpose(0, 1), memory, tgt_mask=tgt_mask, memory_key_padding_mask=src_key_padding_mask,tgt_is_causal=True)\n",
        "      output = self.output_layer(output.transpose(0, 1))\n",
        "      return output"
      ],
      "metadata": {
        "id": "AxIkmSBVW8mJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "ksKqDrypRPQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# resume or start a new graph\n",
        "#init_wandb(False, \"2layers_256d_8heads\")\n",
        "#init_wandb(True, \"2layers_256d_8heads\", resume_run_id=\"pvle5tmk\")\n",
        "\n",
        "# causal mask for auto-regressive\n",
        "tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(max_len)\n",
        "\n",
        "def train_nn_model(model, epochs, optimizer, loss_func):\n",
        "    start_epoch = 0\n",
        "    \"\"\"\n",
        "    # Load checkpoint if available\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Resuming from epoch {start_epoch}\")\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch_num in range(start_epoch, epochs):\n",
        "        print(f\"Epoch: {epoch_num}\")\n",
        "        # train set\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            src, tgt = data[0].to(device), data[1].to(device)\n",
        "            # Exclude last token to ensure prediction of next token (teacher forcing)\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            # Create expected output by shifting target sequences to match predicted output\n",
        "            tgt_expected = tgt[:, 1:].reshape(-1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            src_key_padding_mask = (src == padding_label).to(device)\n",
        "            tgt_key_padding_mask = (tgt_input == padding_label).to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                src,\n",
        "                tgt_input,\n",
        "                src_key_padding_mask=src_key_padding_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                tgt_mask=tgt_mask\n",
        "            )\n",
        "\n",
        "            outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "            loss = loss_func(outputs, tgt_expected)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Train loss: {train_loss}\")\n",
        "\n",
        "        # Evaluate on stratified test set\n",
        "        model.eval()\n",
        "        total_stratified_test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(stratified_test_loader):\n",
        "                src, tgt = data[0].to(device), data[1].to(device)\n",
        "                tgt_input = tgt[:, :-1]\n",
        "                tgt_expected = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "                src_key_padding_mask = (src == padding_label).to(device)\n",
        "                tgt_key_padding_mask = (tgt_input == padding_label).to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    src,\n",
        "                    tgt_input,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                    tgt_mask=tgt_mask\n",
        "                )\n",
        "\n",
        "                outputs = outputs.reshape(-1, outputs.shape[-1])\n",
        "                loss = loss_func(outputs, tgt_expected)\n",
        "                total_stratified_test_loss += loss.item()\n",
        "\n",
        "        stratified_test_loss = total_stratified_test_loss / len(stratified_test_loader)\n",
        "        print(f\"Stratified Test loss: {stratified_test_loss}\\n\")\n",
        "\n",
        "        #wandb_log(train_loss,stratified_test_loss)\n",
        "        #save_checkpoint(epoch_num,model,optimizer)\n",
        "\n",
        "    #wandb.finish()"
      ],
      "metadata": {
        "id": "DoWUN8GtIsqf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training config"
      ],
      "metadata": {
        "id": "9Q4zDauA-J7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0~9 and two special tags\n",
        "toy = Seq2SeqTransformer(12, 12).to(device)\n",
        "loss_func = nn.CrossEntropyLoss(ignore_index=padding_label).to(device)\n",
        "# Neel claims L2 is necessary for grokking to occur\n",
        "optimizer = torch.optim.AdamW(toy.parameters(),lr=1e-4)\n",
        "# Training loop\n",
        "train_nn_model(toy, 3000, optimizer, loss_func)"
      ],
      "metadata": {
        "id": "YQBz63Lc-I3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A report for GCD"
      ],
      "metadata": {
        "id": "V9YXcNEWS84j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#report(toy, train_loader,freq=True)\n",
        "report(toy, stratified_test_loader,freq=True)"
      ],
      "metadata": {
        "id": "4mTHh-TXATL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}